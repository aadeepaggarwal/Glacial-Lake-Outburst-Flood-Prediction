{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "with zipfile.ZipFile(\"GLOFdata.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T19:23:40.699818Z",
     "iopub.status.busy": "2025-04-04T19:23:40.699540Z",
     "iopub.status.idle": "2025-04-04T19:25:33.076643Z",
     "shell.execute_reply": "2025-04-04T19:25:33.075656Z",
     "shell.execute_reply.started": "2025-04-04T19:23:40.699796Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "mydf = pd.read_csv(\"C:\\Users\\aadee\\OneDrive\\Desktop\\GLOF Final\\prepared_dataset.csv\", usecols=[\"lat\", \"lon\", \"mid_date\", \"v [m/yr]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T09:09:34.188239Z",
     "iopub.status.busy": "2025-03-01T09:09:34.187845Z",
     "iopub.status.idle": "2025-03-01T09:09:35.268825Z",
     "shell.execute_reply": "2025-03-01T09:09:35.267956Z",
     "shell.execute_reply.started": "2025-03-01T09:09:34.188207Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data points: 73590934\n",
      "Filtered data points: 4899187\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mid_date</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>v [m/yr]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2022-06-29 05:47:01.220321024</td>\n",
       "      <td>74.353</td>\n",
       "      <td>36.25</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2022-07-04 05:46:39.220404992</td>\n",
       "      <td>74.353</td>\n",
       "      <td>36.25</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2022-07-20 13:05:53.109451008</td>\n",
       "      <td>74.353</td>\n",
       "      <td>36.25</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2022-07-31 17:48:45.220420096</td>\n",
       "      <td>74.353</td>\n",
       "      <td>36.25</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>2022-05-20 05:48:59.220124928</td>\n",
       "      <td>74.353</td>\n",
       "      <td>36.25</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        mid_date     lon    lat  v [m/yr]\n",
       "15 2022-06-29 05:47:01.220321024  74.353  36.25       4.0\n",
       "27 2022-07-04 05:46:39.220404992  74.353  36.25       6.0\n",
       "67 2022-07-20 13:05:53.109451008  74.353  36.25      44.0\n",
       "68 2022-07-31 17:48:45.220420096  74.353  36.25       2.0\n",
       "87 2022-05-20 05:48:59.220124928  74.353  36.25       2.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# First ensure mid_date is a datetime type (if it isn't already)\n",
    "mydf['mid_date'] = pd.to_datetime(mydf['mid_date'])\n",
    "\n",
    "# Filter for dates between 2000-01-01 and 2021-12-31\n",
    "filtered_df = mydf[(mydf['mid_date'] >= '2022-04-01') & (mydf['mid_date'] <= '2022-8-31')]\n",
    "\n",
    "# Display the filtered data\n",
    "print(f\"Original data points: {len(mydf)}\")\n",
    "print(f\"Filtered data points: {len(filtered_df)}\")\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T09:09:36.491078Z",
     "iopub.status.busy": "2025-03-01T09:09:36.490754Z",
     "iopub.status.idle": "2025-03-01T09:09:36.500860Z",
     "shell.execute_reply": "2025-03-01T09:09:36.499836Z",
     "shell.execute_reply.started": "2025-03-01T09:09:36.491052Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mid_date</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>v [m/yr]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [mid_date, lon, lat, v [m/yr]]\n",
       "Index: []"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = filtered_df\n",
    "df[(df[\"v [m/yr]\"] == \"2021-11-15\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T19:25:33.078436Z",
     "iopub.status.busy": "2025-04-04T19:25:33.078121Z",
     "iopub.status.idle": "2025-04-04T19:25:33.654409Z",
     "shell.execute_reply": "2025-04-04T19:25:33.653520Z",
     "shell.execute_reply.started": "2025-04-04T19:25:33.078415Z"
    },
    "id": "ndsMnxHCuBv6",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "# Set up logging to track training metrics\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Check device availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T09:09:38.855791Z",
     "iopub.status.busy": "2025-03-01T09:09:38.855560Z",
     "iopub.status.idle": "2025-03-01T09:09:38.861428Z",
     "shell.execute_reply": "2025-03-01T09:09:38.860645Z",
     "shell.execute_reply.started": "2025-03-01T09:09:38.855773Z"
    },
    "id": "ydAkfmYyyNrt",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Cell # 2\n",
    "# def load_and_preprocess_data(df):\n",
    "#     \"\"\"\n",
    "#     Load and preprocess the dataset for a Transformer model.\n",
    "\n",
    "#     Parameters:\n",
    "#     - file_path (str): Path to the CSV file containing the dataset.\n",
    "\n",
    "#     Returns:\n",
    "#     - data (np.array): Preprocessed data with scaled features and target.\n",
    "#     - scaler_X (StandardScaler): Scaler for the features.\n",
    "#     - scaler_y (StandardScaler): Scaler for the target.\n",
    "#     \"\"\"\n",
    "#     # Load the dataset from CSV\n",
    "#     df = df\n",
    "\n",
    "#     # Drop rows with missing values to ensure data quality\n",
    "#     df = df.dropna()\n",
    "\n",
    "#     # Convert mid_date to datetime\n",
    "#     df['mid_date'] = pd.to_datetime(df['mid_date'])\n",
    "\n",
    "#     # Convert datetime to an ordinal timestamp including time of day\n",
    "#     # Ordinal date + fractional day (hours, minutes, seconds as a fraction of 86400 seconds)\n",
    "#     df['ordinal'] = df['mid_date'].apply(\n",
    "#         lambda x: x.toordinal() + (x.hour * 3600 + x.minute * 60 + x.second) / 86400.0\n",
    "#     )\n",
    "\n",
    "#     # Define features and target\n",
    "#     features = ['ordinal', 'lat', 'lon']\n",
    "#     target = 'v [m/yr]'\n",
    "\n",
    "#     # Extract feature and target arrays\n",
    "#     X = df[features].values\n",
    "#     y = df[target].values\n",
    "\n",
    "#     # Scale features\n",
    "#     scaler_X = StandardScaler()\n",
    "#     X_scaled = scaler_X.fit_transform(X)\n",
    "#     print(X_scaled[:10])\n",
    "\n",
    "\n",
    "#     # Scale target\n",
    "#     scaler_y = StandardScaler()\n",
    "#     y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "#     print(y_scaled[:10])\n",
    "#     # Combine scaled features and target into a single array\n",
    "#     data = np.hstack((X_scaled, y_scaled))\n",
    "\n",
    "#     print(f\"Preprocessed data shape: {data.shape}\")\n",
    "#     print(data[:2])\n",
    "#     return data, scaler_X, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-04T08:31:33.785120Z",
     "iopub.status.busy": "2025-04-04T08:31:33.784804Z",
     "iopub.status.idle": "2025-04-04T08:32:25.657099Z",
     "shell.execute_reply": "2025-04-04T08:32:25.656357Z",
     "shell.execute_reply.started": "2025-04-04T08:31:33.785093Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data type of 'v [m/yr]' before grouping: float64 ---\n",
      "\n",
      "--- Inspecting raw data for group (lon=74.353, lat=36.182, 1990-04-04) ---\n",
      "Number of raw velocity values in this group: 1\n",
      "Raw velocity values:\n",
      "[53.0]\n",
      "Manual Max calculation: 53.0\n",
      "Manual Mean calculation: 53.0\n",
      "--------------------------------------------------------------------------\n",
      "\n",
      "--- Performing Aggregation ---\n",
      "Aggregation complete.\n",
      "\n",
      "--- Checking summary rows where max_velocity != avg_velocity ---\n",
      "Number of summary rows where max_velocity significantly differs from avg_velocity: 9187342\n",
      "Percentage of rows with differing max/avg: 76.79%\n",
      "Example rows where they differ:\n",
      "       lon     lat  year  month  day  max_velocity  avg_velocity\n",
      "14  74.353  36.182  1991      2    6          53.0          39.5\n",
      "23  74.353  36.182  1991      7    8           8.0           6.5\n",
      "24  74.353  36.182  1991      7   16           8.0           5.5\n",
      "25  74.353  36.182  1991      7   24          16.0          12.5\n",
      "33  74.353  36.182  1991     10   28           8.0           6.5\n",
      "---------------------------------------------------------------\n",
      "\n",
      "--- Adding Cyclical Features ---\n",
      "--- Function Complete ---\n",
      "\n",
      "--- Final Resulting Summary DataFrame (Head) ---\n",
      "      lon     lat  year  month  day  max_velocity  avg_velocity     month_sin  \\\n",
      "0  74.353  36.182  1989      5   15           1.0           1.0  5.000000e-01   \n",
      "1  74.353  36.182  1989      7    2           6.0           6.0 -5.000000e-01   \n",
      "2  74.353  36.182  1990      4    4          53.0          53.0  8.660254e-01   \n",
      "3  74.353  36.182  1990      6   11           6.0           6.0  1.224647e-16   \n",
      "4  74.353  36.182  1990      6   20          14.0          14.0  1.224647e-16   \n",
      "\n",
      "   month_cos   day_sin   day_cos  \n",
      "0  -0.866025  0.101168 -0.994869  \n",
      "1  -0.866025  0.394356  0.918958  \n",
      "2  -0.500000  0.743145  0.669131  \n",
      "3  -1.000000  0.743145 -0.669131  \n",
      "4  -1.000000 -0.866025 -0.500000  \n",
      "\n",
      "Shape of the final DataFrame: (11964156, 11)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def investigate_and_summarize_velocity(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Processes a DataFrame to create a daily summary for EACH unique lat/lon pair,\n",
    "    calculating the max and average velocity. Includes detailed investigation steps.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame with 'mid_date', 'lon', 'lat', 'v [m/yr]'.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Summary DataFrame.\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise ValueError(\"Input must be a pandas DataFrame.\")\n",
    "    required_cols = ['mid_date', 'lon', 'lat', 'v [m/yr]']\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        raise ValueError(f\"Input DataFrame missing required columns: {required_cols}\")\n",
    "\n",
    "    # Data Preparation\n",
    "    proc_df = df.copy()\n",
    "    proc_df = proc_df.dropna(subset=required_cols)\n",
    "    try:\n",
    "        proc_df['mid_date'] = pd.to_datetime(proc_df['mid_date'])\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error converting 'mid_date' to datetime: {e}\")\n",
    "\n",
    "    proc_df = proc_df.sort_values(by=['lon', 'lat', 'mid_date'])\n",
    "    proc_df['year'] = proc_df['mid_date'].dt.year\n",
    "    proc_df['month'] = proc_df['mid_date'].dt.month\n",
    "    proc_df['day'] = proc_df['mid_date'].dt.day\n",
    "\n",
    "    # --- Investigation Step 1: Check Data Type ---\n",
    "    print(f\"\\n--- Data type of 'v [m/yr]' before grouping: {proc_df['v [m/yr]'].dtype} ---\")\n",
    "    # Ensure it's numeric, attempt conversion if not (and if sensible)\n",
    "    if not pd.api.types.is_numeric_dtype(proc_df['v [m/yr]']):\n",
    "         print(\"Warning: 'v [m/yr]' is not a numeric type. Attempting conversion.\")\n",
    "         try:\n",
    "             proc_df['v [m/yr]'] = pd.to_numeric(proc_df['v [m/yr]'])\n",
    "             print(f\"--- Data type after conversion: {proc_df['v [m/yr]'].dtype} ---\")\n",
    "         except Exception as e:\n",
    "             raise ValueError(f\"Could not convert 'v [m/yr]' to numeric: {e}\")\n",
    "\n",
    "\n",
    "    # --- Investigation Step 2: Inspect Raw Data for a Specific Group ---\n",
    "    # Using the group from row 2 of your previous output head\n",
    "    inspect_lon, inspect_lat, inspect_year, inspect_month, inspect_day = 74.353, 36.182, 1990, 4, 4\n",
    "    print(f\"\\n--- Inspecting raw data for group (lon={inspect_lon}, lat={inspect_lat}, {inspect_year}-{inspect_month:02d}-{inspect_day:02d}) ---\")\n",
    "    specific_group_filter = (\n",
    "        (proc_df['lon'] == inspect_lon) &\n",
    "        (proc_df['lat'] == inspect_lat) &\n",
    "        (proc_df['year'] == inspect_year) &\n",
    "        (proc_df['month'] == inspect_month) &\n",
    "        (proc_df['day'] == inspect_day)\n",
    "    )\n",
    "    specific_group_velocities = proc_df.loc[specific_group_filter, 'v [m/yr]']\n",
    "\n",
    "    if not specific_group_velocities.empty:\n",
    "        print(f\"Number of raw velocity values in this group: {len(specific_group_velocities)}\")\n",
    "        print(f\"Raw velocity values:\\n{specific_group_velocities.tolist()}\") # Show the actual values\n",
    "        print(f\"Manual Max calculation: {specific_group_velocities.max()}\")\n",
    "        print(f\"Manual Mean calculation: {specific_group_velocities.mean()}\")\n",
    "    else:\n",
    "        print(\"No raw data found for this specific group (check coordinates/date).\")\n",
    "    print(\"--------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    # --- Aggregation (same as before) ---\n",
    "    print(\"\\n--- Performing Aggregation ---\")\n",
    "    loc_daily_summary = proc_df.groupby(['lon', 'lat', 'year', 'month', 'day']).agg(\n",
    "        max_velocity=('v [m/yr]', 'max'),\n",
    "        avg_velocity=('v [m/yr]', 'mean')\n",
    "    ).reset_index()\n",
    "    print(\"Aggregation complete.\")\n",
    "\n",
    "\n",
    "    # --- Investigation Step 3: Check Final Result for Differences ---\n",
    "    print(\"\\n--- Checking summary rows where max_velocity != avg_velocity ---\")\n",
    "    # Use numpy.isclose() for safer float comparison, check if NOT close\n",
    "    diff_rows = loc_daily_summary[~np.isclose(loc_daily_summary['max_velocity'], loc_daily_summary['avg_velocity'])]\n",
    "    num_diff_rows = len(diff_rows)\n",
    "    print(f\"Number of summary rows where max_velocity significantly differs from avg_velocity: {num_diff_rows}\")\n",
    "\n",
    "    if num_diff_rows > 0:\n",
    "        print(f\"Percentage of rows with differing max/avg: {100 * num_diff_rows / len(loc_daily_summary):.2f}%\")\n",
    "        print(\"Example rows where they differ:\")\n",
    "        print(diff_rows.head()) # Show examples where they DO differ\n",
    "    else:\n",
    "        print(\"Still no rows found where max_velocity differs significantly from avg_velocity.\")\n",
    "    print(\"---------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    # --- Optional: Cyclical Feature Encoding (same as before) ---\n",
    "    # ... (code remains the same) ...\n",
    "    print(\"\\n--- Adding Cyclical Features ---\")\n",
    "    loc_daily_summary['month_sin'] = np.sin(2 * np.pi * loc_daily_summary['month'] / 12)\n",
    "    loc_daily_summary['month_cos'] = np.cos(2 * np.pi * loc_daily_summary['month'] / 12)\n",
    "    try:\n",
    "        temp_date_for_daysinmonth = pd.to_datetime(loc_daily_summary[['year', 'month', 'day']])\n",
    "        days_in_month = temp_date_for_daysinmonth.dt.days_in_month\n",
    "        loc_daily_summary['day_sin'] = np.sin(2 * np.pi * loc_daily_summary['day'] / days_in_month)\n",
    "        loc_daily_summary['day_cos'] = np.cos(2 * np.pi * loc_daily_summary['day'] / days_in_month)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not perform daily cyclical encoding. Error: {e}\")\n",
    "        loc_daily_summary['day_sin'] = np.nan\n",
    "        loc_daily_summary['day_cos'] = np.nan\n",
    "\n",
    "\n",
    "    # --- Final Touches (same as before) ---\n",
    "    # ... (code remains the same) ...\n",
    "    final_columns_order = [\n",
    "        'lon', 'lat', 'year', 'month', 'day',\n",
    "        'max_velocity', 'avg_velocity',\n",
    "        'month_sin', 'month_cos', 'day_sin', 'day_cos'\n",
    "    ]\n",
    "    final_columns_order = [col for col in final_columns_order if col in loc_daily_summary.columns]\n",
    "    loc_daily_summary = loc_daily_summary[final_columns_order]\n",
    "    print(\"--- Function Complete ---\")\n",
    "    return loc_daily_summary\n",
    "\n",
    "# Example Usage:\n",
    "# Run this function with your raw DataFrame\n",
    "summary_df = investigate_and_summarize_velocity(mydf)\n",
    "print(\"\\n--- Final Resulting Summary DataFrame (Head) ---\")\n",
    "print(summary_df.head())\n",
    "print(f\"\\nShape of the final DataFrame: {summary_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **OLD CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-04T08:54:45.672451Z",
     "iopub.status.busy": "2025-04-04T08:54:45.672034Z",
     "iopub.status.idle": "2025-04-04T08:54:46.099070Z",
     "shell.execute_reply": "2025-04-04T08:54:46.097977Z",
     "shell.execute_reply.started": "2025-04-04T08:54:45.672418Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>year</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>day_sin</th>\n",
       "      <th>day_cos</th>\n",
       "      <th>max_velocity</th>\n",
       "      <th>avg_velocity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74.353</td>\n",
       "      <td>36.182</td>\n",
       "      <td>1989</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.101168</td>\n",
       "      <td>-0.994869</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74.353</td>\n",
       "      <td>36.182</td>\n",
       "      <td>1989</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.394356</td>\n",
       "      <td>0.918958</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>74.353</td>\n",
       "      <td>36.182</td>\n",
       "      <td>1990</td>\n",
       "      <td>8.660254e-01</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.743145</td>\n",
       "      <td>0.669131</td>\n",
       "      <td>53.0</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>74.353</td>\n",
       "      <td>36.182</td>\n",
       "      <td>1990</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.743145</td>\n",
       "      <td>-0.669131</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74.353</td>\n",
       "      <td>36.182</td>\n",
       "      <td>1990</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11964151</th>\n",
       "      <td>74.540</td>\n",
       "      <td>36.323</td>\n",
       "      <td>2024</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.571268</td>\n",
       "      <td>0.820763</td>\n",
       "      <td>20.0</td>\n",
       "      <td>13.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11964152</th>\n",
       "      <td>74.540</td>\n",
       "      <td>36.323</td>\n",
       "      <td>2024</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.988468</td>\n",
       "      <td>0.151428</td>\n",
       "      <td>43.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11964153</th>\n",
       "      <td>74.540</td>\n",
       "      <td>36.323</td>\n",
       "      <td>2024</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.790776</td>\n",
       "      <td>-0.612106</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11964154</th>\n",
       "      <td>74.540</td>\n",
       "      <td>36.323</td>\n",
       "      <td>2024</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.101168</td>\n",
       "      <td>-0.994869</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11964155</th>\n",
       "      <td>74.540</td>\n",
       "      <td>36.323</td>\n",
       "      <td>2024</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.998717</td>\n",
       "      <td>-0.050649</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11964156 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             lon     lat  year     month_sin  month_cos   day_sin   day_cos  \\\n",
       "0         74.353  36.182  1989  5.000000e-01  -0.866025  0.101168 -0.994869   \n",
       "1         74.353  36.182  1989 -5.000000e-01  -0.866025  0.394356  0.918958   \n",
       "2         74.353  36.182  1990  8.660254e-01  -0.500000  0.743145  0.669131   \n",
       "3         74.353  36.182  1990  1.224647e-16  -1.000000  0.743145 -0.669131   \n",
       "4         74.353  36.182  1990  1.224647e-16  -1.000000 -0.866025 -0.500000   \n",
       "...          ...     ...   ...           ...        ...       ...       ...   \n",
       "11964151  74.540  36.323  2024 -8.660254e-01   0.500000  0.571268  0.820763   \n",
       "11964152  74.540  36.323  2024 -8.660254e-01   0.500000  0.988468  0.151428   \n",
       "11964153  74.540  36.323  2024 -8.660254e-01   0.500000  0.790776 -0.612106   \n",
       "11964154  74.540  36.323  2024 -8.660254e-01   0.500000  0.101168 -0.994869   \n",
       "11964155  74.540  36.323  2024 -8.660254e-01   0.500000 -0.998717 -0.050649   \n",
       "\n",
       "          max_velocity  avg_velocity  \n",
       "0                  1.0           1.0  \n",
       "1                  6.0           6.0  \n",
       "2                 53.0          53.0  \n",
       "3                  6.0           6.0  \n",
       "4                 14.0          14.0  \n",
       "...                ...           ...  \n",
       "11964151          20.0          13.5  \n",
       "11964152          43.0          25.0  \n",
       "11964153           9.0           9.0  \n",
       "11964154          12.0          12.0  \n",
       "11964155          21.0          21.0  \n",
       "\n",
       "[11964156 rows x 9 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mydf1 = summary_df[[\"lon\", 'lat', 'year', 'month_sin', 'month_cos', 'day_sin', 'day_cos', 'max_velocity', 'avg_velocity']]\n",
    "# mydf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T08:55:01.392267Z",
     "iopub.status.busy": "2025-04-04T08:55:01.391937Z",
     "iopub.status.idle": "2025-04-04T08:55:01.397523Z",
     "shell.execute_reply": "2025-04-04T08:55:01.396522Z",
     "shell.execute_reply.started": "2025-04-04T08:55:01.392231Z"
    },
    "id": "0USRKfGFyQMj",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Cell 3: Define the TimeSeriesDataset class\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for time series data.\"\"\"\n",
    "    def __init__(self, data, seq_length):\n",
    "        self.data = data.astype(np.float32)\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return sequence of features and target.\"\"\"\n",
    "        X = self.data[idx:idx + self.seq_length, :-1]  # All features except target\n",
    "        y = self.data[idx + self.seq_length - 1, -1]   # Last velocity value in sequence\n",
    "        return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T08:55:07.178313Z",
     "iopub.status.busy": "2025-04-04T08:55:07.178000Z",
     "iopub.status.idle": "2025-04-04T08:55:07.184625Z",
     "shell.execute_reply": "2025-04-04T08:55:07.183541Z",
     "shell.execute_reply.started": "2025-04-04T08:55:07.178288Z"
    },
    "id": "3vU_kHEryKFS",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Cell 4: Define the Transformer model\n",
    "class TransformerRegressor(nn.Module):\n",
    "    \"\"\"Transformer model for velocity prediction.\"\"\"\n",
    "    def __init__(self, input_dim, d_model, nhead, num_layers, dropout=0.1):\n",
    "        super(TransformerRegressor, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = nn.Parameter(torch.zeros(1, 1000, d_model))  # Positional encoding\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(d_model, 1)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize model weights.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the model.\"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        x = self.embedding(x)  # [batch_size, seq_len, d_model]\n",
    "        x = x + self.pos_encoder[:, :seq_len, :]  # Add positional encoding\n",
    "        x = self.transformer_encoder(x)  # [batch_size, seq_len, d_model]\n",
    "        x = self.fc(x[:, -1, :])  # Predict from last time step\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T09:09:43.263540Z",
     "iopub.status.busy": "2025-03-01T09:09:43.263220Z",
     "iopub.status.idle": "2025-03-01T09:09:43.272773Z",
     "shell.execute_reply": "2025-03-01T09:09:43.271952Z",
     "shell.execute_reply.started": "2025-03-01T09:09:43.263512Z"
    },
    "id": "ettnh0rjyH9U",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Cell 5: Training function with metric tracking\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def count_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Count and return the number of trainable parameters in the model.\n",
    "    \n",
    "    Parameters:\n",
    "    - model (nn.Module): The PyTorch model.\n",
    "    \n",
    "    Returns:\n",
    "    - int: Number of trainable parameters.\n",
    "    \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, scaler, num_epochs):\n",
    "    \"\"\"Train the model and track metrics.\"\"\"\n",
    "    # Print the number of trainable parameters before starting training\n",
    "    trainable_params = count_trainable_parameters(model)\n",
    "    print(f\"Number of trainable parameters: {trainable_params}\")\n",
    "    \n",
    "    # Lists to store metrics for plotting later\n",
    "    train_losses, val_losses, val_maes = [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        pbar_train = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "        for X, y in pbar_train:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # Updated autocast syntax to kill that warning\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                output = model(X)\n",
    "                loss = criterion(output.squeeze(), y)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            train_loss += loss.item()\n",
    "            # Show the batch loss in the progress bar\n",
    "            pbar_train.set_postfix({'loss': loss.item()})\n",
    "\n",
    "        # Average train loss for the epoch\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, val_mae = 0, 0\n",
    "        pbar_val = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "        with torch.no_grad():\n",
    "            for X, y in pbar_val:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    output = model(X)\n",
    "                loss = criterion(output.squeeze(), y)\n",
    "                val_loss += loss.item()\n",
    "                val_mae += torch.mean(torch.abs(output.squeeze() - y)).item()\n",
    "                # Show batch loss and MAE in the progress bar\n",
    "                pbar_val.set_postfix({'loss': loss.item(), 'MAE': val_mae / len(val_loader)})\n",
    "\n",
    "        # Average val loss and MAE for the epoch\n",
    "        val_loss /= len(val_loader)\n",
    "        val_mae /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        val_maes.append(val_mae)\n",
    "\n",
    "        # Print all the metrics after each epoch\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val MAE: {val_mae:.4f}\")\n",
    "\n",
    "        # Step the scheduler based on val loss\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "    # Return the metrics for plotting or analysis\n",
    "    return train_losses, val_losses, val_maes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-04T08:55:10.292623Z",
     "iopub.status.busy": "2025-04-04T08:55:10.292328Z",
     "iopub.status.idle": "2025-04-04T08:55:10.297647Z",
     "shell.execute_reply": "2025-04-04T08:55:10.296896Z",
     "shell.execute_reply.started": "2025-04-04T08:55:10.292600Z"
    },
    "id": "QsdhacGtxgYg",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "6781c329-1d58-4a85-d46e-412d0f0bade8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q119Sy31vR_x",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "a543b791-64f7-4876-b8c2-62e3b5684a4d"
   },
   "outputs": [],
   "source": [
    "# Cell 6: Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters (adjustable for experimentation)\n",
    "    SEQ_LENGTH = 32       # Sequence length\n",
    "    BATCH_SIZE = 1024         # Batch size\n",
    "    NUM_EPOCHS = 10          # Number of epochs\n",
    "    LEARNING_RATE = 0.001    # Learning rate\n",
    "    D_MODEL = 256            # Model dimension\n",
    "    NHEAD = 8                # Number of attention heads\n",
    "    NUM_LAYERS = 4           # Number of transformer layers\n",
    "    DROPOUT = 0.2           # Dropout rate\n",
    "\n",
    "    # Load and preprocess data\n",
    "    # file_path = '/kaggle/input/df-raw-fydp/df_raw.csv'\n",
    "    # file_path = '/home/ubuntu/data/df_raw.csv'\n",
    "\n",
    "    data, scaler_X, scaler_y = load_and_preprocess_data(mydf1)\n",
    "\n",
    "    # Split data into train, validation, and test sets\n",
    "    train_size = int(0.7 * len(data))\n",
    "    val_size = int(0.2 * len(data))\n",
    "    train_data = data[:train_size]\n",
    "    val_data = data[train_size:train_size + val_size]\n",
    "    test_data = data\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = TimeSeriesDataset(train_data, SEQ_LENGTH)\n",
    "    val_dataset = TimeSeriesDataset(val_data, SEQ_LENGTH)\n",
    "    test_dataset = TimeSeriesDataset(test_data, SEQ_LENGTH)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "    # Initialize model, loss, optimizer, and scheduler\n",
    "    model = TransformerRegressor(\n",
    "        input_dim=3, d_model=D_MODEL, nhead=NHEAD, num_layers=NUM_LAYERS, dropout=DROPOUT\n",
    "    ).to(device)\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n",
    "    scaler = GradScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-02-28T15:51:49.126524Z",
     "iopub.status.busy": "2025-02-28T15:51:49.126328Z"
    },
    "id": "llHmep51x3mK",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "outputId": "a6c0cd65-f237-4e5c-a036-c83e7f7e95ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 2500737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]: 100%|██████████| 30124/30124 [1:09:01<00:00,  7.27it/s, loss=0.941]\n",
      "Epoch 1/10 [Val]: 100%|██████████| 8607/8607 [05:56<00:00, 24.11it/s, loss=0.292, MAE=0.417] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train Loss: 1.0262, Val Loss: 0.8748, Val MAE: 0.4167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Train]:  51%|█████▏    | 15497/30124 [35:41<33:35,  7.26it/s, loss=0.694] "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "if(True):\n",
    "    train_losses, val_losses, val_maes = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, scheduler, scaler, NUM_EPOCHS\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *NEW CODE***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T09:04:48.224270Z",
     "iopub.status.busy": "2025-04-04T09:04:48.223912Z",
     "iopub.status.idle": "2025-04-04T09:05:40.349576Z",
     "shell.execute_reply": "2025-04-04T09:05:40.348808Z",
     "shell.execute_reply.started": "2025-04-04T09:04:48.224243Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.cuda.amp import GradScaler\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Assuming 'device' is already defined (e.g., device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def investigate_and_summarize_velocity(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Processes a DataFrame to create a daily summary for EACH unique lat/lon pair,\n",
    "    calculating the max and average velocity. Includes detailed investigation steps.\n",
    "    \"\"\"\n",
    "    # ... (rest of the investigate_and_summarize_velocity function remains the same) ...\n",
    "    proc_df = df.copy()\n",
    "    proc_df = proc_df.dropna(subset=['mid_date', 'lon', 'lat', 'v [m/yr]'])\n",
    "    try:\n",
    "        proc_df['mid_date'] = pd.to_datetime(proc_df['mid_date'])\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error converting 'mid_date' to datetime: {e}\")\n",
    "\n",
    "    proc_df = proc_df.sort_values(by=['lon', 'lat', 'mid_date'])\n",
    "    proc_df['year'] = proc_df['mid_date'].dt.year\n",
    "    proc_df['month'] = proc_df['mid_date'].dt.month\n",
    "    proc_df['day'] = proc_df['mid_date'].dt.day\n",
    "\n",
    "    if not pd.api.types.is_numeric_dtype(proc_df['v [m/yr]']):\n",
    "        print(\"Warning: 'v [m/yr]' is not a numeric type. Attempting conversion.\")\n",
    "        try:\n",
    "            proc_df['v [m/yr]'] = pd.to_numeric(proc_df['v [m/yr]'])\n",
    "            print(f\"--- Data type after conversion: {proc_df['v [m/yr]'].dtype} ---\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Could not convert 'v [m/yr]' to numeric: {e}\")\n",
    "\n",
    "    loc_daily_summary = proc_df.groupby(['lon', 'lat', 'year', 'month', 'day']).agg(\n",
    "        max_velocity=('v [m/yr]', 'max'),\n",
    "        avg_velocity=('v [m/yr]', 'mean')\n",
    "    ).reset_index()\n",
    "\n",
    "    loc_daily_summary['month_sin'] = np.sin(2 * np.pi * loc_daily_summary['month'] / 12)\n",
    "    loc_daily_summary['month_cos'] = np.cos(2 * np.pi * loc_daily_summary['month'] / 12)\n",
    "    try:\n",
    "        temp_date_for_daysinmonth = pd.to_datetime(loc_daily_summary[['year', 'month', 'day']])\n",
    "        days_in_month = temp_date_for_daysinmonth.dt.days_in_month\n",
    "        loc_daily_summary['day_sin'] = np.sin(2 * np.pi * loc_daily_summary['day'] / days_in_month)\n",
    "        loc_daily_summary['day_cos'] = np.cos(2 * np.pi * loc_daily_summary['day'] / days_in_month)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not perform daily cyclical encoding. Error: {e}\")\n",
    "        loc_daily_summary['day_sin'] = np.nan\n",
    "        loc_daily_summary['day_cos'] = np.nan\n",
    "\n",
    "    final_columns_order = [\n",
    "        'lon', 'lat', 'year', 'month_sin', 'month_cos', 'day_sin', 'day_cos', 'max_velocity'\n",
    "    ]\n",
    "    final_columns_order = [col for col in final_columns_order if col in loc_daily_summary.columns]\n",
    "    loc_daily_summary = loc_daily_summary[final_columns_order]\n",
    "    return loc_daily_summary\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for time series data.\"\"\"\n",
    "    def __init__(self, data, seq_length):\n",
    "        self.data = data.astype(np.float32)\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return sequence of features and target.\"\"\"\n",
    "        X = self.data[idx:idx + self.seq_length, :-1]  # All features except target\n",
    "        y = self.data[idx + self.seq_length - 1, -1]   # Last max_velocity value in sequence\n",
    "        return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "class TransformerRegressor(nn.Module):\n",
    "    \"\"\"Transformer model for velocity prediction.\"\"\"\n",
    "    def __init__(self, input_dim, d_model, nhead, num_layers, dropout=0.1):\n",
    "        super(TransformerRegressor, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = nn.Parameter(torch.zeros(1, 1000, d_model))  # Positional encoding\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(d_model, 1)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize model weights.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the model.\"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        x = self.embedding(x)  # [batch_size, seq_len, d_model]\n",
    "        x = x + self.pos_encoder[:, :seq_len, :]  # Add positional encoding\n",
    "        x = self.transformer_encoder(x)  # [batch_size, seq_len, d_model]\n",
    "        x = self.fc(x[:, -1, :])  # Predict from last time step\n",
    "        return x\n",
    "\n",
    "def count_trainable_parameters(model):\n",
    "    \"\"\"Count and return the number of trainable parameters in the model.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, scaler, num_epochs):\n",
    "    \"\"\"Train the model and track metrics.\"\"\"\n",
    "    trainable_params = count_trainable_parameters(model)\n",
    "    print(f\"Number of trainable parameters: {trainable_params}\")\n",
    "\n",
    "    train_losses, val_losses, val_maes = [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        pbar_train = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "        for X, y in pbar_train:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                output = model(X)\n",
    "                loss = criterion(output.squeeze(), y)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            train_loss += loss.item()\n",
    "            pbar_train.set_postfix({'loss': loss.item()})\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_mae = 0, 0\n",
    "        pbar_val = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "        with torch.no_grad():\n",
    "            for X, y in pbar_val:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    output = model(X)\n",
    "                loss = criterion(output.squeeze(), y)\n",
    "                val_loss += loss.item()\n",
    "                val_mae += torch.mean(torch.abs(output.squeeze() - y)).item()\n",
    "                pbar_val.set_postfix({'loss': loss.item(), 'MAE': val_mae / len(val_loader)})\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_mae /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        val_maes.append(val_mae)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val MAE: {val_mae:.4f}\")\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "    return train_losses, val_losses, val_maes\n",
    "\n",
    "# Example Usage (replace 'mydf' with your DataFrame):\n",
    "# mydf = pd.read_csv(\"your_data.csv\") #replace with your data loading.\n",
    "summary_df = investigate_and_summarize_velocity(mydf)\n",
    "\n",
    "data = summary_df[[ 'lon', 'lat', 'year', 'month_sin', 'month_cos', 'day_sin', 'day_cos', 'max_velocity']].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T19:25:33.656132Z",
     "iopub.status.busy": "2025-04-04T19:25:33.655755Z",
     "iopub.status.idle": "2025-04-04T19:26:48.953605Z",
     "shell.execute_reply": "2025-04-04T19:26:48.952613Z",
     "shell.execute_reply.started": "2025-04-04T19:25:33.656098Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data type of 'v [m/yr]' before grouping: float64 ---\n",
      "\n",
      "--- Inspecting raw data for group (lon=74.353, lat=36.182, 1990-04-04) ---\n",
      "Number of raw velocity values in this group: 1\n",
      "Raw velocity values:\n",
      "[53.0]\n",
      "Manual Max calculation: 53.0\n",
      "Manual Mean calculation: 53.0\n",
      "--------------------------------------------------------------------------\n",
      "\n",
      "--- Checking summary rows where max_velocity != avg_velocity ---\n",
      "Number of summary rows where max_velocity significantly differs from avg_velocity: 9187342\n",
      "Percentage of rows with differing max/avg: 76.79%\n",
      "Example rows where they differ:\n",
      "       lon     lat  year  month  day  max_velocity  avg_velocity\n",
      "14  74.353  36.182  1991      2    6          53.0          39.5\n",
      "23  74.353  36.182  1991      7    8           8.0           6.5\n",
      "24  74.353  36.182  1991      7   16           8.0           5.5\n",
      "25  74.353  36.182  1991      7   24          16.0          12.5\n",
      "33  74.353  36.182  1991     10   28           8.0           6.5\n",
      "---------------------------------------------------------------\n",
      "\n",
      "--- Adding Cyclical Features ---\n",
      "--- Function Complete ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "# Assuming 'device' is already defined (e.g., device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def investigate_and_summarize_velocity(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Processes a DataFrame to create a daily summary for EACH unique lat/lon pair,\n",
    "    calculating the max and average velocity. Includes detailed investigation steps.\n",
    "    \"\"\"\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise ValueError(\"Input must be a pandas DataFrame.\")\n",
    "    required_cols = ['mid_date', 'lon', 'lat', 'v [m/yr]']\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        raise ValueError(f\"Input DataFrame missing required columns: {required_cols}\")\n",
    "\n",
    "    proc_df = df.copy()\n",
    "    proc_df = proc_df.dropna(subset=required_cols)\n",
    "    try:\n",
    "        proc_df['mid_date'] = pd.to_datetime(proc_df['mid_date'])\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error converting 'mid_date' to datetime: {e}\")\n",
    "\n",
    "    proc_df = proc_df.sort_values(by=['lon', 'lat', 'mid_date'])\n",
    "    proc_df['year'] = proc_df['mid_date'].dt.year\n",
    "    proc_df['month'] = proc_df['mid_date'].dt.month\n",
    "    proc_df['day'] = proc_df['mid_date'].dt.day\n",
    "\n",
    "    print(f\"\\n--- Data type of 'v [m/yr]' before grouping: {proc_df['v [m/yr]'].dtype} ---\")\n",
    "    if not pd.api.types.is_numeric_dtype(proc_df['v [m/yr]']):\n",
    "        print(\"Warning: 'v [m/yr]' is not a numeric type. Attempting conversion.\")\n",
    "        try:\n",
    "            proc_df['v [m/yr]'] = pd.to_numeric(proc_df['v [m/yr]'])\n",
    "            print(f\"--- Data type after conversion: {proc_df['v [m/yr]'].dtype} ---\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Could not convert 'v [m/yr]' to numeric: {e}\")\n",
    "\n",
    "    inspect_lon, inspect_lat, inspect_year, inspect_month, inspect_day = 74.353, 36.182, 1990, 4, 4\n",
    "    print(f\"\\n--- Inspecting raw data for group (lon={inspect_lon}, lat={inspect_lat}, {inspect_year}-{inspect_month:02d}-{inspect_day:02d}) ---\")\n",
    "    specific_group_filter = (\n",
    "        (proc_df['lon'] == inspect_lon) &\n",
    "        (proc_df['lat'] == inspect_lat) &\n",
    "        (proc_df['year'] == inspect_year) &\n",
    "        (proc_df['month'] == inspect_month) &\n",
    "        (proc_df['day'] == inspect_day)\n",
    "    )\n",
    "    specific_group_velocities = proc_df.loc[specific_group_filter, 'v [m/yr]']\n",
    "\n",
    "    if not specific_group_velocities.empty:\n",
    "        print(f\"Number of raw velocity values in this group: {len(specific_group_velocities)}\")\n",
    "        print(f\"Raw velocity values:\\n{specific_group_velocities.tolist()}\")\n",
    "        print(f\"Manual Max calculation: {specific_group_velocities.max()}\")\n",
    "        print(f\"Manual Mean calculation: {specific_group_velocities.mean()}\")\n",
    "    else:\n",
    "        print(\"No raw data found for this specific group (check coordinates/date).\")\n",
    "    print(\"--------------------------------------------------------------------------\")\n",
    "\n",
    "    loc_daily_summary = proc_df.groupby(['lon', 'lat', 'year', 'month', 'day']).agg(\n",
    "        max_velocity=('v [m/yr]', 'max'),\n",
    "        avg_velocity=('v [m/yr]', 'mean')\n",
    "    ).reset_index()\n",
    "\n",
    "    print(\"\\n--- Checking summary rows where max_velocity != avg_velocity ---\")\n",
    "    diff_rows = loc_daily_summary[~np.isclose(loc_daily_summary['max_velocity'], loc_daily_summary['avg_velocity'])]\n",
    "    num_diff_rows = len(diff_rows)\n",
    "    print(f\"Number of summary rows where max_velocity significantly differs from avg_velocity: {num_diff_rows}\")\n",
    "\n",
    "    if num_diff_rows > 0:\n",
    "        print(f\"Percentage of rows with differing max/avg: {100 * num_diff_rows / len(loc_daily_summary):.2f}%\")\n",
    "        print(\"Example rows where they differ:\")\n",
    "        print(diff_rows.head())\n",
    "    else:\n",
    "        print(\"Still no rows found where max_velocity differs significantly from avg_velocity.\")\n",
    "    print(\"---------------------------------------------------------------\")\n",
    "\n",
    "    print(\"\\n--- Adding Cyclical Features ---\")\n",
    "    loc_daily_summary['month_sin'] = np.sin(2 * np.pi * loc_daily_summary['month'] / 12)\n",
    "    loc_daily_summary['month_cos'] = np.cos(2 * np.pi * loc_daily_summary['month'] / 12)\n",
    "    try:\n",
    "        temp_date_for_daysinmonth = pd.to_datetime(loc_daily_summary[['year', 'month', 'day']])\n",
    "        days_in_month = temp_date_for_daysinmonth.dt.days_in_month\n",
    "        loc_daily_summary['day_sin'] = np.sin(2 * np.pi * loc_daily_summary['day'] / days_in_month)\n",
    "        loc_daily_summary['day_cos'] = np.cos(2 * np.pi * loc_daily_summary['day'] / days_in_month)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not perform daily cyclical encoding. Error: {e}\")\n",
    "        loc_daily_summary['day_sin'] = np.nan\n",
    "        loc_daily_summary['day_cos'] = np.nan\n",
    "\n",
    "    final_columns_order = [\n",
    "        'lon', 'lat', 'year', 'month_sin', 'month_cos', 'day_sin', 'day_cos', 'avg_velocity', 'max_velocity'\n",
    "    ]\n",
    "    final_columns_order = [col for col in final_columns_order if col in loc_daily_summary.columns]\n",
    "    loc_daily_summary = loc_daily_summary[final_columns_order]\n",
    "    print(\"--- Function Complete ---\")\n",
    "    return loc_daily_summary\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for time series data.\"\"\"\n",
    "    def __init__(self, data, seq_length):\n",
    "        self.data = data.astype(np.float32)\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return sequence of features and target.\"\"\"\n",
    "        X = self.data[idx:idx + self.seq_length, :-1]\n",
    "        y = self.data[idx + self.seq_length - 1, -1]\n",
    "        return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "class TransformerRegressor(nn.Module):\n",
    "    \"\"\"Transformer model for velocity prediction.\"\"\"\n",
    "    def __init__(self, input_dim, d_model, nhead, num_layers, dropout=0.1):\n",
    "        super(TransformerRegressor, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = nn.Parameter(torch.zeros(1, 1000, d_model))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(d_model, 1)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        x = self.embedding(x)\n",
    "        x = x + self.pos_encoder[:, :seq_len, :]\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        return x\n",
    "\n",
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, scaler, num_epochs):\n",
    "    trainable_params = count_trainable_parameters(model)\n",
    "    print(f\"Number of trainable parameters: {trainable_params}\")\n",
    "\n",
    "    train_losses, val_losses, val_maes = [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        pbar_train = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "        for X, y in pbar_train:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                output = model(X)\n",
    "                loss = criterion(output.squeeze(), y)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            train_loss += loss.item()\n",
    "            pbar_train.set_postfix({'loss': loss.item()})\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_mae = 0, 0\n",
    "        pbar_val = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "        with torch.no_grad():\n",
    "            for X, y in pbar_val:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    output = model(X)\n",
    "                loss = criterion(output.squeeze(), y)\n",
    "                val_loss += loss.item()\n",
    "                val_mae += torch.mean(torch.abs(output.squeeze() - y)).item()\n",
    "                pbar_val.set_postfix({'loss': loss.item(), 'MAE': val_mae / len(val_loader)})\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_mae /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        val_maes.append(val_mae)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val MAE: {val_mae:.4f}\")\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "    return train_losses, val_losses, val_maes\n",
    "\n",
    "# Example Usage:\n",
    "# mydf = pd.read_csv(\"your_data.csv\") # Replace with your data loading\n",
    "summary_df = investigate_and_summarize_velocity(mydf)\n",
    "\n",
    "data = summary_df[[ 'lon', 'lat', 'year', 'month_sin', 'month_cos', 'day_sin', 'day_cos', 'avg_velocity', 'max_velocity']].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T19:26:48.954915Z",
     "iopub.status.busy": "2025-04-04T19:26:48.954684Z",
     "iopub.status.idle": "2025-04-04T19:26:51.322182Z",
     "shell.execute_reply": "2025-04-04T19:26:51.321340Z",
     "shell.execute_reply.started": "2025-04-04T19:26:48.954897Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-882f9a42a925>:29: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "SEQ_LENGTH = 32\n",
    "BATCH_SIZE = 2048\n",
    "NUM_EPOCHS = 30\n",
    "LEARNING_RATE = 0.00001\n",
    "D_MODEL = 256\n",
    "NHEAD = 8\n",
    "NUM_LAYERS = 4\n",
    "DROPOUT = 0.2\n",
    "\n",
    "train_size = int(0.7 * len(data))\n",
    "val_size = int(0.2 * len(data))\n",
    "train_data = data[:train_size]\n",
    "val_data = data[train_size:train_size + val_size]\n",
    "test_data = data[train_size + val_size:]\n",
    "\n",
    "\n",
    "train_dataset = TimeSeriesDataset(train_data, SEQ_LENGTH)\n",
    "val_dataset = TimeSeriesDataset(val_data, SEQ_LENGTH)\n",
    "test_dataset = TimeSeriesDataset(test_data, SEQ_LENGTH)\n",
    "                               \n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "model = TransformerRegressor(input_dim=8, d_model=D_MODEL, nhead=NHEAD, num_layers=NUM_LAYERS, dropout=DROPOUT).to(device) # input_dim is 8\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n",
    "scaler = GradScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T19:27:45.944456Z",
     "iopub.status.busy": "2025-04-04T19:27:45.944119Z",
     "iopub.status.idle": "2025-04-04T19:27:45.948510Z",
     "shell.execute_reply": "2025-04-04T19:27:45.947879Z",
     "shell.execute_reply.started": "2025-04-04T19:27:45.944428Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 5518849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 [Train]: 100%|██████████| 4090/4090 [09:10<00:00,  7.42it/s, loss=40.6]\n",
      "Epoch 1/30 [Val]: 100%|██████████| 1169/1169 [00:40<00:00, 28.98it/s, loss=25.2, MAE=37.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - Train Loss: 40.4021, Val Loss: 37.8494, Val MAE: 37.8494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 [Train]: 100%|██████████| 4090/4090 [09:12<00:00,  7.41it/s, loss=32.4]\n",
      "Epoch 2/30 [Val]: 100%|██████████| 1169/1169 [00:40<00:00, 28.67it/s, loss=18.2, MAE=29.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 - Train Loss: 36.9007, Val Loss: 29.6484, Val MAE: 29.6484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 [Train]: 100%|██████████| 4090/4090 [09:11<00:00,  7.41it/s, loss=26.3]\n",
      "Epoch 3/30 [Val]: 100%|██████████| 1169/1169 [00:40<00:00, 28.70it/s, loss=16.8, MAE=27.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 - Train Loss: 31.8588, Val Loss: 27.5255, Val MAE: 27.5255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 [Train]: 100%|██████████| 4090/4090 [09:11<00:00,  7.42it/s, loss=29.7]\n",
      "Epoch 4/30 [Val]: 100%|██████████| 1169/1169 [00:40<00:00, 28.70it/s, loss=16.5, MAE=26]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 - Train Loss: 29.4904, Val Loss: 25.9766, Val MAE: 25.9766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 [Train]: 100%|██████████| 4090/4090 [09:11<00:00,  7.42it/s, loss=23.5]\n",
      "Epoch 5/30 [Val]: 100%|██████████| 1169/1169 [00:40<00:00, 28.69it/s, loss=16.4, MAE=24.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 - Train Loss: 28.0398, Val Loss: 24.8303, Val MAE: 24.8303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 [Train]: 100%|██████████| 4090/4090 [09:10<00:00,  7.42it/s, loss=27.7]\n",
      "Epoch 6/30 [Val]: 100%|██████████| 1169/1169 [00:40<00:00, 28.66it/s, loss=17, MAE=24.4]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 - Train Loss: 26.9616, Val Loss: 24.4316, Val MAE: 24.4316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 [Train]: 100%|██████████| 4090/4090 [09:10<00:00,  7.42it/s, loss=24.4]\n",
      "Epoch 7/30 [Val]: 100%|██████████| 1169/1169 [00:40<00:00, 28.67it/s, loss=15.5, MAE=23.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 - Train Loss: 26.0754, Val Loss: 23.0516, Val MAE: 23.0516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 [Train]: 100%|██████████| 4090/4090 [09:10<00:00,  7.43it/s, loss=26.2]\n",
      "Epoch 8/30 [Val]: 100%|██████████| 1169/1169 [00:40<00:00, 28.69it/s, loss=15.6, MAE=22.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 - Train Loss: 25.3647, Val Loss: 22.6392, Val MAE: 22.6392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 [Train]: 100%|██████████| 4090/4090 [09:10<00:00,  7.42it/s, loss=24.7]\n",
      "Epoch 9/30 [Val]: 100%|██████████| 1169/1169 [00:40<00:00, 28.64it/s, loss=15.6, MAE=22.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 - Train Loss: 24.8631, Val Loss: 22.5656, Val MAE: 22.5656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 [Train]: 100%|██████████| 4090/4090 [09:10<00:00,  7.43it/s, loss=25.9]\n",
      "Epoch 10/30 [Val]: 100%|██████████| 1169/1169 [00:40<00:00, 28.72it/s, loss=14.7, MAE=21.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 - Train Loss: 24.4160, Val Loss: 21.7032, Val MAE: 21.7032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 [Train]: 100%|██████████| 4090/4090 [09:10<00:00,  7.44it/s, loss=23.3]\n",
      "Epoch 11/30 [Val]: 100%|██████████| 1169/1169 [00:40<00:00, 28.71it/s, loss=15.9, MAE=22.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 - Train Loss: 24.0532, Val Loss: 22.5000, Val MAE: 22.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 [Train]: 100%|██████████| 4090/4090 [09:10<00:00,  7.43it/s, loss=21]  \n",
      "Epoch 12/30 [Val]: 100%|██████████| 1169/1169 [00:40<00:00, 28.69it/s, loss=15.8, MAE=22.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 - Train Loss: 23.7443, Val Loss: 22.3252, Val MAE: 22.3252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 [Train]: 100%|██████████| 4090/4090 [09:10<00:00,  7.44it/s, loss=24.1]\n",
      "Epoch 13/30 [Val]: 100%|██████████| 1169/1169 [00:40<00:00, 28.66it/s, loss=15.3, MAE=21.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 - Train Loss: 23.5285, Val Loss: 21.4232, Val MAE: 21.4232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 [Train]: 100%|██████████| 4090/4090 [09:09<00:00,  7.44it/s, loss=19.8]\n",
      "Epoch 14/30 [Val]: 100%|██████████| 1169/1169 [00:40<00:00, 28.72it/s, loss=14.3, MAE=20.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 - Train Loss: 23.2585, Val Loss: 20.9307, Val MAE: 20.9307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 [Train]: 100%|██████████| 4090/4090 [09:09<00:00,  7.44it/s, loss=20.3]\n",
      "Epoch 15/30 [Val]: 100%|██████████| 1169/1169 [00:40<00:00, 28.68it/s, loss=16.4, MAE=22.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 - Train Loss: 23.0872, Val Loss: 22.0885, Val MAE: 22.0885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 [Train]: 100%|██████████| 4090/4090 [09:09<00:00,  7.44it/s, loss=22.6]\n",
      "Epoch 16/30 [Val]: 100%|██████████| 1169/1169 [00:40<00:00, 28.73it/s, loss=14.6, MAE=20.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 - Train Loss: 22.8290, Val Loss: 20.3625, Val MAE: 20.3625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 [Train]: 100%|██████████| 4090/4090 [09:09<00:00,  7.44it/s, loss=22.1]\n",
      "Epoch 17/30 [Val]: 100%|██████████| 1169/1169 [00:40<00:00, 28.70it/s, loss=15.4, MAE=21]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 - Train Loss: 22.6950, Val Loss: 20.9751, Val MAE: 20.9751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 [Train]: 100%|██████████| 4090/4090 [09:09<00:00,  7.44it/s, loss=24.6]\n",
      "Epoch 18/30 [Val]: 100%|██████████| 1169/1169 [00:40<00:00, 28.71it/s, loss=14.6, MAE=20.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 - Train Loss: 22.5158, Val Loss: 20.4091, Val MAE: 20.4091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 [Train]: 100%|██████████| 4090/4090 [09:09<00:00,  7.44it/s, loss=25.3]\n",
      "Epoch 19/30 [Val]: 100%|██████████| 1169/1169 [00:40<00:00, 28.73it/s, loss=15, MAE=20.4]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 - Train Loss: 22.3762, Val Loss: 20.4209, Val MAE: 20.4209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 [Train]: 100%|██████████| 4090/4090 [09:09<00:00,  7.44it/s, loss=21.2]\n",
      "Epoch 20/30 [Val]: 100%|██████████| 1169/1169 [00:40<00:00, 28.73it/s, loss=14.4, MAE=20.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 - Train Loss: 21.7884, Val Loss: 20.1331, Val MAE: 20.1331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 [Train]: 100%|██████████| 4090/4090 [09:09<00:00,  7.45it/s, loss=22.7]\n",
      "Epoch 21/30 [Val]: 100%|██████████| 1169/1169 [00:40<00:00, 28.72it/s, loss=15.1, MAE=20.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 - Train Loss: 21.7393, Val Loss: 20.7208, Val MAE: 20.7208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 [Train]: 100%|██████████| 4090/4090 [09:09<00:00,  7.45it/s, loss=20.1]\n",
      "Epoch 22/30 [Val]: 100%|██████████| 1169/1169 [00:40<00:00, 28.73it/s, loss=14.4, MAE=20.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 - Train Loss: 21.6972, Val Loss: 20.1861, Val MAE: 20.1861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 [Train]: 100%|██████████| 4090/4090 [09:08<00:00,  7.46it/s, loss=19.9]\n",
      "Epoch 23/30 [Val]: 100%|██████████| 1169/1169 [00:40<00:00, 28.76it/s, loss=14.4, MAE=20]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 - Train Loss: 21.6646, Val Loss: 20.0386, Val MAE: 20.0386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 [Train]: 100%|██████████| 4090/4090 [09:09<00:00,  7.45it/s, loss=22.1]\n",
      "Epoch 24/30 [Val]: 100%|██████████| 1169/1169 [00:40<00:00, 28.77it/s, loss=14.4, MAE=20.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 - Train Loss: 21.6151, Val Loss: 20.0733, Val MAE: 20.0733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 [Train]: 100%|██████████| 4090/4090 [09:09<00:00,  7.45it/s, loss=22.5]\n",
      "Epoch 25/30 [Val]: 100%|██████████| 1169/1169 [00:40<00:00, 28.76it/s, loss=14.8, MAE=20.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 - Train Loss: 21.5809, Val Loss: 20.5544, Val MAE: 20.5544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 [Train]: 100%|██████████| 4090/4090 [09:09<00:00,  7.45it/s, loss=20.5]\n",
      "Epoch 26/30 [Val]: 100%|██████████| 1169/1169 [00:40<00:00, 28.72it/s, loss=14.9, MAE=20.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 - Train Loss: 21.5470, Val Loss: 20.3067, Val MAE: 20.3067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 [Train]: 100%|██████████| 4090/4090 [09:09<00:00,  7.44it/s, loss=21.9]\n",
      "Epoch 27/30 [Val]: 100%|██████████| 1169/1169 [00:40<00:00, 28.76it/s, loss=14.6, MAE=20]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 - Train Loss: 21.4531, Val Loss: 20.0394, Val MAE: 20.0394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 [Train]: 100%|██████████| 4090/4090 [09:09<00:00,  7.45it/s, loss=20.6]\n",
      "Epoch 28/30 [Val]: 100%|██████████| 1169/1169 [00:40<00:00, 28.72it/s, loss=14.6, MAE=20.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 - Train Loss: 21.4501, Val Loss: 20.0521, Val MAE: 20.0521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 [Train]: 100%|██████████| 4090/4090 [09:09<00:00,  7.44it/s, loss=21.1]\n",
      "Epoch 29/30 [Val]: 100%|██████████| 1169/1169 [00:40<00:00, 28.73it/s, loss=14.7, MAE=20.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 - Train Loss: 21.4411, Val Loss: 20.0843, Val MAE: 20.0843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 [Train]: 100%|██████████| 4090/4090 [09:09<00:00,  7.44it/s, loss=20.5]\n",
      "Epoch 30/30 [Val]: 100%|██████████| 1169/1169 [00:40<00:00, 28.73it/s, loss=14.6, MAE=20]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 - Train Loss: 21.4270, Val Loss: 20.0454, Val MAE: 20.0454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses, val_maes = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, scaler, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "execution": {
     "iopub.execute_input": "2025-04-04T19:27:51.569109Z",
     "iopub.status.busy": "2025-04-04T19:27:51.568806Z",
     "iopub.status.idle": "2025-04-04T19:30:11.016962Z",
     "shell.execute_reply": "2025-04-04T19:30:11.016055Z",
     "shell.execute_reply.started": "2025-04-04T19:27:51.569085Z"
    },
    "id": "Uk_S1FPz_EmO",
    "outputId": "70311177-f21c-4bc6-fa23-eef159786f46",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 585/585 [02:19<00:00,  4.20it/s]\n"
     ]
    }
   ],
   "source": [
    "if(True):\n",
    "    # # Cell 7: Plot training metrics\n",
    "    # plt.figure(figsize=(12, 4))\n",
    "    # plt.subplot(1, 2, 1)\n",
    "    # plt.plot(train_losses, label='Train Loss')\n",
    "    # plt.plot(val_losses, label='Val Loss')\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.ylabel('Loss')\n",
    "    # plt.legend()\n",
    "    # plt.title('Training and Validation Loss')\n",
    "\n",
    "    # plt.subplot(1, 2, 2)\n",
    "    # plt.plot(val_maes, label='Val MAE')\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.ylabel('MAE')\n",
    "    # plt.legend()\n",
    "    # plt.title('Validation MAE')\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "    # Cell 8: Evaluate on test set\n",
    "    model.eval()\n",
    "    test_loss, test_mae = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in tqdm(test_loader, desc=\"Testing\"):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            output = model(X)\n",
    "            test_loss += criterion(output.squeeze(), y).item()\n",
    "            test_mae += torch.mean(torch.abs(output.squeeze() - y)).item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_mae /= len(test_loader)\n",
    "    logger.info(f\"Test Loss: {test_loss:.4f}, Test MAE: {test_mae:.4f}\")\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), 'transformer_model.pth')\n",
    "    logger.info(\"Model saved as 'transformer_model.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T19:31:34.727608Z",
     "iopub.status.busy": "2025-04-04T19:31:34.727315Z",
     "iopub.status.idle": "2025-04-04T19:31:34.732453Z",
     "shell.execute_reply": "2025-04-04T19:31:34.731582Z",
     "shell.execute_reply.started": "2025-04-04T19:31:34.727586Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.53790953220465"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T19:31:38.646619Z",
     "iopub.status.busy": "2025-04-04T19:31:38.646282Z",
     "iopub.status.idle": "2025-04-04T19:31:38.686466Z",
     "shell.execute_reply": "2025-04-04T19:31:38.685575Z",
     "shell.execute_reply.started": "2025-04-04T19:31:38.646593Z"
    },
    "id": "WSPTeGIMFFEY",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-d9195ccf9604>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"/kaggle/input/terraflow-5.5m/pytorch/default/1/terraflow_transformer_model.pth\", map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerRegressor(\n",
       "  (embedding): Linear(in_features=8, out_features=256, bias=True)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model state dict\n",
    "model.load_state_dict(torch.load(\"/kaggle/input/terraflow-5.5m/pytorch/default/1/terraflow_transformer_model.pth\", map_location=device))\n",
    "model.eval()  # Set to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T19:33:12.685529Z",
     "iopub.status.busy": "2025-04-04T19:33:12.685187Z",
     "iopub.status.idle": "2025-04-04T19:33:13.450026Z",
     "shell.execute_reply": "2025-04-04T19:33:13.449037Z",
     "shell.execute_reply.started": "2025-04-04T19:33:12.685503Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "predictions = []\n",
    "actuals = []\n",
    "n = 50  # Number of predictions to make\n",
    "i = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Convert to numpy and store\n",
    "        for j in range(len(outputs)):\n",
    "            if i >= n:  # Stop after 10 predictions\n",
    "                break\n",
    "            predictions.append(outputs[j].cpu().numpy())\n",
    "            actuals.append(targets[j].cpu().numpy())\n",
    "            i += 1\n",
    "        if i >= n:\n",
    "            break  # Exit the outer loop once we have 10 samples\n",
    "\n",
    "# Convert to numpy arrays\n",
    "predictions = np.array(predictions)\n",
    "actuals = np.array(actuals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T19:33:20.006766Z",
     "iopub.status.busy": "2025-04-04T19:33:20.006468Z",
     "iopub.status.idle": "2025-04-04T19:33:20.012826Z",
     "shell.execute_reply": "2025-04-04T19:33:20.011843Z",
     "shell.execute_reply.started": "2025-04-04T19:33:20.006743Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unscaled Predictions: [[11.595757 ]\n",
      " [12.968755 ]\n",
      " [21.906458 ]\n",
      " [27.22831  ]\n",
      " [27.846622 ]\n",
      " [68.2304   ]\n",
      " [ 9.040334 ]\n",
      " [18.85722  ]\n",
      " [ 8.690692 ]\n",
      " [25.8592   ]\n",
      " [24.048616 ]\n",
      " [ 8.062716 ]\n",
      " [26.703175 ]\n",
      " [ 7.4819183]\n",
      " [37.59923  ]\n",
      " [10.675871 ]\n",
      " [34.841415 ]\n",
      " [18.002289 ]\n",
      " [ 9.848359 ]\n",
      " [62.418068 ]\n",
      " [28.0903   ]\n",
      " [ 7.867091 ]\n",
      " [ 9.150924 ]\n",
      " [21.424036 ]\n",
      " [10.582641 ]\n",
      " [22.73966  ]\n",
      " [22.346846 ]\n",
      " [18.705713 ]\n",
      " [71.562195 ]\n",
      " [47.122406 ]\n",
      " [19.106821 ]\n",
      " [16.405764 ]\n",
      " [34.697266 ]\n",
      " [21.49151  ]\n",
      " [19.906448 ]\n",
      " [38.107635 ]\n",
      " [35.52249  ]\n",
      " [44.588303 ]\n",
      " [37.209545 ]\n",
      " [21.709034 ]\n",
      " [31.761469 ]\n",
      " [32.700367 ]\n",
      " [32.852226 ]\n",
      " [42.487335 ]\n",
      " [22.93914  ]\n",
      " [53.386208 ]\n",
      " [35.226692 ]\n",
      " [18.724316 ]\n",
      " [23.551628 ]\n",
      " [30.846554 ]]\n",
      "Unscaled Actuals: [  9.  14.  23.  51.  17.  17.  13.  15.  11.  62.  20.  12.  30.  12.\n",
      "   9.  20.  32.  19.   9.  15.  80.   9.  23.  16.   9.  48.  22.  28.\n",
      "  21.  60.  24.  15.  22.  10.  27.  96.  27. 119.  33.  20.  56.  29.\n",
      "  42.  39.  21. 109.  32.  19.  19.  27.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Unscaled Predictions:\", predictions[:])\n",
    "print(\"Unscaled Actuals:\", actuals[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T09:13:59.149663Z",
     "iopub.status.busy": "2025-03-01T09:13:59.149316Z",
     "iopub.status.idle": "2025-03-01T09:13:59.155001Z",
     "shell.execute_reply": "2025-03-01T09:13:59.154288Z",
     "shell.execute_reply.started": "2025-03-01T09:13:59.149636Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.31663486,  0.00221978, -0.21034998, -0.1040651 , -0.05092266],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actuals[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T09:31:38.830741Z",
     "iopub.status.busy": "2025-03-01T09:31:38.830401Z",
     "iopub.status.idle": "2025-03-01T09:31:38.853273Z",
     "shell.execute_reply": "2025-03-01T09:31:38.852531Z",
     "shell.execute_reply.started": "2025-03-01T09:31:38.830713Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mid_date</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>v [m/yr]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40101287</th>\n",
       "      <td>2022-04-04 01:07:06.856773120</td>\n",
       "      <td>74.4637</td>\n",
       "      <td>36.317</td>\n",
       "      <td>3867.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              mid_date      lon     lat  v [m/yr]\n",
       "40101287 2022-04-04 01:07:06.856773120  74.4637  36.317    3867.0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['v [m/yr]'] == df['v [m/yr]'].max())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T09:33:46.913625Z",
     "iopub.status.busy": "2025-03-01T09:33:46.913321Z",
     "iopub.status.idle": "2025-03-01T09:33:47.147738Z",
     "shell.execute_reply": "2025-03-01T09:33:47.146756Z",
     "shell.execute_reply.started": "2025-03-01T09:33:46.913581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Input Tensor: tensor([[-1.6271,  1.5344,  0.3538]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def to_ordinal(x):\n",
    "    \"\"\"Converts datetime string to ordinal format with fractional day.\"\"\"\n",
    "    x = pd.to_datetime(x)  # Convert string to datetime\n",
    "    return x.toordinal() + (x.hour * 3600 + x.minute * 60 + x.second) / 86400.0\n",
    "\n",
    "# Define your input values\n",
    "time = '2022-04-04 01:07:06.856773120'\n",
    "lat = 36.317\n",
    "lon = 74.4637\n",
    "\n",
    "# Convert time to ordinal\n",
    "ordinal_time = to_ordinal(time)\n",
    "\n",
    "# Convert to a NumPy array\n",
    "new_input = np.array([[ordinal_time, lat, lon]])\n",
    "\n",
    "# Scale using the same scaler used for training\n",
    "new_input_scaled = scaler_X.transform(new_input)\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "new_input_tensor = torch.tensor(new_input_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "print(\"Processed Input Tensor:\", new_input_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T09:36:09.070660Z",
     "iopub.status.busy": "2025-03-01T09:36:09.070300Z",
     "iopub.status.idle": "2025-03-01T09:36:09.079106Z",
     "shell.execute_reply": "2025-03-01T09:36:09.078427Z",
     "shell.execute_reply.started": "2025-03-01T09:36:09.070623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Velocity: 7.9246 m/yr\n"
     ]
    }
   ],
   "source": [
    "# Reshape input to (batch_size=1, seq_length=1, feature_dim=3)\n",
    "new_input_tensor = new_input_tensor.unsqueeze(0)  \n",
    "\n",
    "with torch.no_grad():\n",
    "    vel_predicted = model(new_input_tensor).cpu().numpy()  # Convert tensor to NumPy\n",
    "\n",
    "# Inverse transform to get the original velocity value\n",
    "vel_predicted_original = scaler_y.inverse_transform(vel_predicted.reshape(-1, 1))\n",
    "\n",
    "print(f\"Predicted Velocity: {vel_predicted_original[0][0]:.4f} m/yr\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6723786,
     "sourceId": 10828484,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 291823,
     "modelInstanceId": 270835,
     "sourceId": 321229,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
